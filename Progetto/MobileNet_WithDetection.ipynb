{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MobileNet_WithDetection.ipynb","provenance":[{"file_id":"1RVGD-s_HhPZ5YELN1GyhWqY67Gy_3b8S","timestamp":1577700128764},{"file_id":"1ppwqSZQq5D4GVw2vDXngUNOdUpj3Un13","timestamp":1577543790190},{"file_id":"15IvlS60iW6RTKQBIco4fzNCOmfU9Gyib","timestamp":1577458301828}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VC5rdmuNSya2","colab_type":"code","outputId":"ddfba3c5-7c9c-414d-c68b-10655fb92fd9","executionInfo":{"status":"ok","timestamp":1577700204964,"user_tz":-60,"elapsed":50030,"user":{"displayName":"Giovanni Longobardi","photoUrl":"","userId":"11225905338379198690"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content')\n","\n","\n","!unzip -q drive/My\\ Drive/dataset.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1jAKOJx0UiQn","colab_type":"code","colab":{}},"source":["import cv2\n","import random\n","\n","import numpy as np\n","import keras\n","from PIL import Image\n","\n","from keras.layers.pooling import MaxPooling2D,AveragePooling2D\n","from keras.optimizers import SGD, Adam\n","from keras.callbacks import ModelCheckpoint \n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import LearningRateScheduler \n","\n","#choose the images size\n","img_size_row=224\n","img_size_column=224\n","global IMAGE_SIZE\n","IMAGE_SIZE=(img_size_row , img_size_column)\n","\n","#configuration for training\n","train_size= 9621\n","test_size= 1463\n","val_size= 2720\n","\n","#choose the batch size\n","batch_size=64\n","steps_per_epoch=train_size*5/batch_size\n","validation_steps=val_size*5/batch_size\n","\n","#choose the epochs\n","epochs=50\n","\n","#num of classes to be trained\n","num_classes=32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8pI1Hs0x-GP","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","def data_aug(img):\n","  #Brightess\n","  if random.randint(0,1):    \n","     exposure = random.randint(1,2)\n","     if exposure==2:\n","         brightess=img*0.7\n","     else:\n","         brightess=img*1.4\n","     brightess = np.clip(brightess, 0, 255)\n","     img = brightess.astype(np.uint8)\n","\n","  #Gaussian noise\n","  if random.randint(0,1):\n","     row,col,ch= img.shape\n","     binom=np.random.binomial(n=500, p=0.5,size=(row,col,ch))\n","     binom=binom-250\n","     noisy = img + binom\n","     noisy = np.clip(noisy, 0, 255)\n","     img = noisy.astype(np.uint8)\n","\n","  #Flip\n","  if random.randint(0,1):\n","     flip=np.fliplr(img)\n","     img=flip\n","\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XC9sicFKhZq","colab_type":"code","colab":{}},"source":["def generator(path,txt,num_classes,batch_size=16):\n","  f = open(txt,\"r\")\n","  \n","  ds=[]\n","  for row in f:\n","    img, classe, s = row.split(\";\",3)\n","    ds.append( (img,classe) )\n","\n","  dataset=np.array(ds)\n","\n","  while True:\n","    np.random.shuffle(dataset)\n","    img1_list=[]\n","    label = []\n","\n","    for i in dataset:\n","      img_1 = i[0]\n","\n","      img_1 = cv2.imread(path+\"/\"+img_1)\n","\n","      if(img_1 is None):\n","        print(img)\n","\n","      img_1 = data_aug(np.array(img_1))\n","      img_1 = cv2.resize(img_1, (224,224) ,0,0, cv2.INTER_LINEAR)\n","      img_1 = img_1.astype(np.float32)\n","      img_1 /= 255\n","      img1_list.append(img_1)\n","      label.append(int(i[1]))\n","\n","      if len(label)==batch_size:\n","        array1 = np.array(img1_list)\n","        label_categorical=keras.utils.to_categorical(label,num_classes=num_classes)\n","        lab = np.array(label_categorical)\n","        img1_list=[]\n","        label = []\n","        yield (array1 , lab)          "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3WOG23iWs_v","colab_type":"code","colab":{}},"source":["from keras.layers import Dense, Activation, Flatten, Dropout\n","from keras.models import Sequential, Model\n","\n","def build_finetune_model(base_model, dropout, num_classes):\n","\n","    x = base_model.output\n","    \n","    x = AveragePooling2D((5, 5), name='avg_pool')(x)\n","    x = Flatten()(x)\n","    x = Dropout(dropout)(x)\n","    predictions = Dense(num_classes, activation='softmax', name='finalfc')(x)\n","    \n","    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n","\n","    return finetune_model\n","\n","dropout = 0.25\n","\n","base_model = keras.applications.mobilenet.MobileNet(weights='imagenet',\n","                            include_top=False,\n","                            input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1], 3))\n","\n","finetune_model = build_finetune_model(base_model,\n","                                      dropout=dropout,\n","                                      num_classes=num_classes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TndRd2cfdLOh","colab_type":"code","colab":{}},"source":["learning_rate_di_base=0.0001\n","learning_rate_decay_factor=0.05\n","learning_rate_decay_epochs=10\n","def step_decay_schedule(initial_lr=learning_rate_di_base, decay_factor=learning_rate_decay_factor, step_size=learning_rate_decay_epochs):\n","    '''\n","    Wrapper function to create a LearningRateScheduler with step decay schedule.\n","    '''\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n","    \n","    return LearningRateScheduler(schedule,verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQqZ5hd7dBO9","colab_type":"code","colab":{}},"source":["#train\n","finetune_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#callbacks\n","filepath=\"MobileNet_yesDetection.hdf5\"\n","lr_sched = step_decay_schedule()\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [EarlyStopping(monitor='val_acc',\n","                           patience=10,\n","                           verbose=1,\n","                           min_delta=1e-4,\n","                           mode='max'),\n","                  lr_sched,\n","                  checkpoint]\n","                  \n","#---------------------------ok\n","\n","print(finetune_model.summary())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ulFncA-chPW","colab_type":"code","colab":{}},"source":["path=\"dataset\"\n","\n","t_gen = generator(path,\"dataset/file_path_train.txt\",num_classes,batch_size=batch_size)\n","v_gen = generator(path,\"dataset/file_path_val.txt\",num_classes,batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrfiOi54dVJV","colab_type":"code","colab":{}},"source":["finetune_model.fit_generator(t_gen,steps_per_epoch=steps_per_epoch , epochs=epochs, verbose=1,callbacks=callbacks_list,\n","                               validation_data=v_gen, validation_steps=validation_steps,use_multiprocessing=True,workers = 4)"],"execution_count":0,"outputs":[]}]}